{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4511b2c3",
   "metadata": {},
   "source": [
    "# Colab 2: LoRA Finetuning with SmolLM2 135M\n",
    "## Parameter-Efficient Fine-tuning using Unsloth\n",
    "\n",
    "This notebook demonstrates **LoRA (Low-Rank Adaptation)** fine-tuning of the SmolLM2 135M model.\n",
    "\n",
    "### What is LoRA?\n",
    "- **LoRA**: Updates only a small subset of parameters (adapters)\n",
    "- **Memory Efficient**: Requires much less VRAM than full finetuning\n",
    "- **Faster Training**: Trains quicker with fewer parameters\n",
    "- **Good Performance**: Achieves ~95-99% of full finetuning quality\n",
    "- **Easy to Share**: LoRA adapters are small (~10-50MB vs GBs)\n",
    "\n",
    "### Key Differences from Colab 1 (Full Finetuning):\n",
    "| Feature | Full Finetuning | LoRA Finetuning |\n",
    "|---------|----------------|------------------|\n",
    "| Parameters Updated | ALL (135M) | ~1-2M (1-2%) |\n",
    "| Memory Usage | High | Low |\n",
    "| Training Speed | Slower | Faster |\n",
    "| Adapter Size | Full model | ~10-50MB |\n",
    "| r parameter | 0 (not used) | 16-64 |\n",
    "| lora_alpha | 0 (not used) | 16-32 |\n",
    "\n",
    "### Dataset:\n",
    "We'll use the same Alpaca dataset for fair comparison with Colab 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d8590",
   "metadata": {},
   "source": [
    "## Step 1: Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8fc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install --upgrade datasets transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a59663",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44bfe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bade6c0",
   "metadata": {},
   "source": [
    "## Step 3: Configure Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model_name = \"unsloth/SmolLM2-135M-Instruct\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(f\"  Max Sequence Length: {max_seq_length}\")\n",
    "print(f\"  4-bit Quantization: {load_in_4bit}\")\n",
    "print(f\"  Training Mode: LoRA FINETUNING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c22dcb9",
   "metadata": {},
   "source": [
    "## Step 4: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ace2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15687a1e",
   "metadata": {},
   "source": [
    "## Step 5: Configure LoRA Parameters\n",
    "\n",
    "### LoRA Parameters Explained:\n",
    "\n",
    "**r (rank)**: \n",
    "- Controls the size of LoRA adapters\n",
    "- Higher = more parameters = better performance but slower\n",
    "- Typical values: 8, 16, 32, 64\n",
    "- We use **16** for a good balance\n",
    "\n",
    "**lora_alpha**: \n",
    "- Scaling factor for LoRA weights\n",
    "- Usually set to r or 2*r\n",
    "- We use **16** (same as r)\n",
    "\n",
    "**lora_dropout**: \n",
    "- Prevents overfitting\n",
    "- 0 = no dropout, 0.1 = 10% dropout\n",
    "- We use **0** for small datasets\n",
    "\n",
    "**target_modules**: \n",
    "- Which layers to apply LoRA to\n",
    "- More modules = more parameters\n",
    "- We target all attention and MLP layers\n",
    "\n",
    "**Key Difference from Full Finetuning**: r > 0 enables LoRA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2897131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank - KEY PARAMETER!\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,  # LoRA scaling factor\n",
    "    lora_dropout=0,  # Dropout for regularization\n",
    "    bias=\"none\",  # Don't add bias\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # Rank-stabilized LoRA\n",
    "    loftq_config=None,  # LoftQ quantization\n",
    ")\n",
    "\n",
    "print(\"Model configured for LoRA FINETUNING!\")\n",
    "print(f\"\\nLoRA Configuration:\")\n",
    "print(f\"  Rank (r): 16\")\n",
    "print(f\"  Alpha: 16\")\n",
    "print(f\"  Dropout: 0\")\n",
    "print(f\"  Target modules: 7 (all attention + MLP layers)\")\n",
    "print(f\"\\nOnly ~1-2% of parameters will be updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c280ac",
   "metadata": {},
   "source": [
    "## Step 6: Load and Prepare Dataset\n",
    "\n",
    "Using the same Alpaca dataset as Colab 1 for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2312551",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:1000]\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"\\nSample data point:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e09545",
   "metadata": {},
   "source": [
    "## Step 7: Format Dataset with Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"<|im_start|>system\n",
    "You are a helpful AI assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}{input}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{output}<|im_end|>\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        input_section = f\"\\n\\nInput: {input_text}\" if input_text else \"\"\n",
    "        text = chat_template.format(\n",
    "            instruction=instruction,\n",
    "            input=input_section,\n",
    "            output=output\n",
    "        ) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(\"Dataset formatted successfully!\")\n",
    "print(f\"\\nExample formatted text:\")\n",
    "print(dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7992029",
   "metadata": {},
   "source": [
    "## Step 8: Configure Training Arguments\n",
    "\n",
    "### Key Differences from Full Finetuning:\n",
    "- **learning_rate**: 2e-4 (can be higher for LoRA, sometimes 1e-3 to 5e-4)\n",
    "- **Training is faster** due to fewer parameters\n",
    "- **Less memory usage** overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4baf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,  # Same as full finetuning for comparison\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Trainer configured successfully!\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Batch size: 2\")\n",
    "print(f\"  Gradient accumulation: 4\")\n",
    "print(f\"  Effective batch size: 8\")\n",
    "print(f\"  Max steps: 60\")\n",
    "print(f\"  Learning rate: 2e-4\")\n",
    "print(f\"  Training mode: LoRA (Parameter-Efficient)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55e31b",
   "metadata": {},
   "source": [
    "## Step 9: Start Training!\n",
    "\n",
    "Training only ~1-2M parameters with LoRA adapters.\n",
    "This should be noticeably faster than full finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69455f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nStarting LoRA finetuning...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show statistics\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "training_time = trainer_stats.metrics['train_runtime']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LoRA Training completed successfully!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Memory used for training = {used_memory_for_training} GB.\")\n",
    "print(f\"Percentage of max memory used = {used_percentage}%\")\n",
    "print(f\"Training time = {training_time:.2f} seconds\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nðŸ’¡ Compare this with Full Finetuning from Colab 1!\")\n",
    "print(f\"   LoRA uses less memory and trains faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b5b746",
   "metadata": {},
   "source": [
    "## Step 10: Test the LoRA Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_prompts = [\n",
    "    \"Explain what machine learning is in simple terms.\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"What are the benefits of exercise?\"\n",
    "]\n",
    "\n",
    "print(\"Testing the LoRA fine-tuned model...\\n\")\n",
    "\n",
    "for i, instruction in enumerate(test_prompts, 1):\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "You are a helpful AI assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test {i}: {instruction}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16afbff",
   "metadata": {},
   "source": [
    "## Step 11: Save the LoRA Adapters\n",
    "\n",
    "### Advantage of LoRA:\n",
    "LoRA adapters are tiny (~10-50MB) compared to full models (GBs)!\n",
    "You can easily share them or switch between multiple adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters only\n",
    "model.save_pretrained(\"smollm2_135m_lora_adapters\")\n",
    "tokenizer.save_pretrained(\"smollm2_135m_lora_adapters\")\n",
    "\n",
    "print(\"LoRA adapters saved to 'smollm2_135m_lora_adapters/'\")\n",
    "print(\"\\nâœ¨ LoRA adapters are much smaller than full models!\")\n",
    "print(\"   You can easily share or switch between multiple adapters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752f9c4",
   "metadata": {},
   "source": [
    "## Step 12: (Optional) Merge LoRA with Base Model\n",
    "\n",
    "You can merge the LoRA adapters into the base model to create a standalone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042cf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapters with base model\n",
    "# Uncomment to run:\n",
    "\n",
    "# model.save_pretrained_merged(\"smollm2_135m_lora_merged\", tokenizer, save_method=\"merged_16bit\")\n",
    "# print(\"Merged model saved to 'smollm2_135m_lora_merged/'\")\n",
    "\n",
    "# Or save in different formats:\n",
    "# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\")  # 4-bit\n",
    "# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\")  # LoRA only\n",
    "\n",
    "print(\"To merge LoRA with base model, uncomment the code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb97cd",
   "metadata": {},
   "source": [
    "## Step 13: (Optional) Export to GGUF for Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152846c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GGUF format\n",
    "# Uncomment the quantization method you want:\n",
    "\n",
    "# Q4_K_M - recommended\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "# Q8_0 - high quality\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q8_0\")\n",
    "\n",
    "# F16 - full precision\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n",
    "\n",
    "print(\"To export to GGUF, uncomment one of the lines above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1742d",
   "metadata": {},
   "source": [
    "## Summary & Comparison\n",
    "\n",
    "### What We Did:\n",
    "1. âœ… Installed unsloth\n",
    "2. âœ… Loaded SmolLM2 135M model\n",
    "3. âœ… Configured **LoRA parameters** (r=16, alpha=16)\n",
    "4. âœ… Trained only ~1-2% of parameters\n",
    "5. âœ… Tested the model\n",
    "6. âœ… Saved tiny LoRA adapters (~10-50MB)\n",
    "\n",
    "### Full Finetuning (Colab 1) vs LoRA (Colab 2):\n",
    "\n",
    "| Metric | Full Finetuning | LoRA Finetuning |\n",
    "|--------|----------------|------------------|\n",
    "| **Parameters Updated** | 135M (100%) | ~1-2M (1-2%) |\n",
    "| **Memory Usage** | Higher | Lower |\n",
    "| **Training Speed** | Slower | Faster |\n",
    "| **Adapter Size** | Full model (GBs) | ~10-50MB |\n",
    "| **Performance** | 100% (baseline) | 95-99% |\n",
    "| **Best For** | Critical tasks | Most tasks |\n",
    "\n",
    "### LoRA Configuration:\n",
    "```python\n",
    "r = 16  # Rank - higher = more parameters\n",
    "lora_alpha = 16  # Scaling factor\n",
    "lora_dropout = 0  # Regularization\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                  \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "```\n",
    "\n",
    "### When to Use LoRA:\n",
    "- âœ… Limited GPU memory\n",
    "- âœ… Need fast training\n",
    "- âœ… Want to share adapters easily\n",
    "- âœ… Good enough performance for most tasks\n",
    "- âœ… Want to maintain multiple task-specific adapters\n",
    "\n",
    "### When to Use Full Finetuning:\n",
    "- âœ… Critical tasks requiring maximum performance\n",
    "- âœ… Have sufficient GPU memory and time\n",
    "- âœ… Domain is very different from pre-training\n",
    "\n",
    "### Tips for Better LoRA Results:\n",
    "1. **Increase r**: Try r=32 or r=64 for better performance\n",
    "2. **Adjust alpha**: Usually alpha = r or alpha = 2*r\n",
    "3. **More data**: Use more training samples\n",
    "4. **More steps**: Increase max_steps to 500-1000\n",
    "5. **Learning rate**: Experiment with 1e-4 to 5e-4"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
