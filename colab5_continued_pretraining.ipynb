{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b82e689",
   "metadata": {},
   "source": [
    "# Colab 5: Continued Pretraining\n",
    "## Domain Adaptation through Continued Pretraining\n",
    "\n",
    "This notebook demonstrates **Continued Pretraining** - teaching a pre-trained model new domain knowledge.\n",
    "\n",
    "### What is Continued Pretraining?\n",
    "- **Continued Pretraining**: Further training a pre-trained model on domain-specific raw text\n",
    "- **Purpose**: Adapt the model to a specific domain (code, medical, legal, etc.)\n",
    "- **Different from Finetuning**: Uses raw text, not instruction-response pairs\n",
    "- **Goal**: Expand model's knowledge in a particular area\n",
    "\n",
    "### Key Differences:\n",
    "| Method | Data Format | Goal |\n",
    "|--------|-------------|------|\n",
    "| **Pretraining** | Raw text | Learn language |\n",
    "| **Continued Pretraining** | Domain-specific raw text | Learn domain knowledge |\n",
    "| **Finetuning** | Instruction-response pairs | Learn to follow instructions |\n",
    "| **LoRA Finetuning** | Instruction-response pairs | Efficient instruction following |\n",
    "\n",
    "### Use Cases:\n",
    "1. **Code Models**: Train on GitHub code repositories\n",
    "2. **Medical Models**: Train on medical literature and journals\n",
    "3. **Legal Models**: Train on legal documents and case law\n",
    "4. **Multilingual**: Add new language capabilities\n",
    "5. **Recent Events**: Update model with new information\n",
    "\n",
    "### In This Notebook:\n",
    "We'll adapt SmolLM2 to become better at Python programming by continuing pretraining on Python code from the `code_search_net` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9774c08",
   "metadata": {},
   "source": [
    "## Step 1: Install Unsloth\n",
    "\n",
    "**Important**: We need to use `datasets==4.3.0` to avoid recursion errors with Unsloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0baa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Use datasets==4.3.0 to avoid recursion errors\n",
    "!pip install datasets==4.3.0\n",
    "!pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d93922",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797110e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70bbf1c",
   "metadata": {},
   "source": [
    "## Step 3: Configure Model Parameters\n",
    "\n",
    "### Continued Pretraining Settings:\n",
    "- **Higher learning rate**: 1e-4 to 5e-4 (vs 2e-4 for finetuning)\n",
    "- **Longer training**: More epochs to learn domain knowledge\n",
    "- **Packing**: Enable for efficiency with varying text lengths\n",
    "- **No instruction template**: We use raw text, not instruction-response format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6692fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "max_seq_length = 2048\n",
    "dtype = None  # Auto-detect\n",
    "load_in_4bit = True  # Use 4bit quantization\n",
    "\n",
    "# Using SmolLM2 135M for faster training\n",
    "model_name = \"unsloth/SmolLM2-135M-Instruct\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(f\"  Max Sequence Length: {max_seq_length}\")\n",
    "print(f\"  4-bit Quantization: {load_in_4bit}\")\n",
    "print(f\"  Training Mode: CONTINUED PRETRAINING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e67377",
   "metadata": {},
   "source": [
    "## Step 4: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81faf813",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018f340",
   "metadata": {},
   "source": [
    "## Step 5: Configure for Continued Pretraining\n",
    "\n",
    "### Options:\n",
    "\n",
    "**Option A: Full Continued Pretraining (All Parameters)**\n",
    "- Updates ALL 135M parameters\n",
    "- Best for major domain adaptation\n",
    "- More memory intensive\n",
    "\n",
    "**Option B: LoRA Continued Pretraining**\n",
    "- Updates only adapter parameters\n",
    "- Memory efficient\n",
    "- Good for moderate domain adaptation\n",
    "\n",
    "We'll use **Option B (LoRA)** for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for continued pretraining\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  # Higher rank for pretraining (16-64)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,  # Typically equals r for pretraining\n",
    "    lora_dropout=0.05,  # Small dropout\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"Model configured for CONTINUED PRETRAINING!\")\n",
    "print(\"Using LoRA adapters for memory efficiency.\")\n",
    "print(f\"LoRA rank: 32 (higher than finetuning for better adaptation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2e6d3",
   "metadata": {},
   "source": [
    "## Step 6: Load Domain-Specific Dataset\n",
    "\n",
    "### Dataset: CodeSearchNet (Python)\n",
    "- Contains Python code snippets and documentation\n",
    "- Raw code text (not instruction-response pairs)\n",
    "- Perfect for teaching coding abilities\n",
    "\n",
    "### Alternative Datasets:\n",
    "- **Medical**: PubMed abstracts, medical journals\n",
    "- **Legal**: Legal documents, case law\n",
    "- **News**: Recent news articles\n",
    "- **Books**: Domain-specific literature\n",
    "- **Wikipedia**: General knowledge updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7493fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CodeSearchNet dataset (Python subset)\n",
    "dataset = load_dataset(\n",
    "    \"code_search_net\",\n",
    "    \"python\",\n",
    "    split=\"train[:5000]\",  # Using 5000 samples for demo\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"\\nSample data point:\")\n",
    "print(dataset[0])\n",
    "print(f\"\\nColumns: {dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec3bf08",
   "metadata": {},
   "source": [
    "## Step 7: Prepare Raw Text for Pretraining\n",
    "\n",
    "### Key Differences from Finetuning:\n",
    "1. **No chat template**: Just raw text\n",
    "2. **No instruction format**: Not question-answer pairs\n",
    "3. **Use function + docstring**: Complete code context\n",
    "4. **Add EOS token**: Mark end of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8914ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset for continued pretraining\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_func(examples):\n",
    "    \"\"\"Format code examples for continued pretraining.\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for func_name, docstring, code in zip(\n",
    "        examples[\"func_name\"],\n",
    "        examples[\"func_documentation_string\"],\n",
    "        examples[\"whole_func_string\"]\n",
    "    ):\n",
    "        # Create a complete code snippet with docstring\n",
    "        # This teaches the model both code and documentation\n",
    "        if docstring and code:\n",
    "            text = f\"# Python Code Example\\n\\n{code}\\n\\n# Documentation:\\n{docstring}{EOS_TOKEN}\"\n",
    "        elif code:\n",
    "            text = f\"# Python Code Example\\n\\n{code}{EOS_TOKEN}\"\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(\n",
    "    formatting_func,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names  # Remove original columns\n",
    ")\n",
    "\n",
    "# Filter out empty entries\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 50)\n",
    "\n",
    "print(f\"Dataset formatted: {len(dataset)} samples\")\n",
    "print(f\"\\nExample formatted text:\")\n",
    "print(dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553626f",
   "metadata": {},
   "source": [
    "## Step 8: Configure Training Arguments\n",
    "\n",
    "### Continued Pretraining Settings:\n",
    "- **Higher learning rate**: `1e-4` (vs `2e-4` for finetuning)\n",
    "- **More epochs**: `3` epochs to learn domain knowledge\n",
    "- **Enable packing**: Efficiently use sequence length\n",
    "- **Longer training**: More steps than finetuning\n",
    "\n",
    "### Why Different from Finetuning?\n",
    "- Learning new knowledge requires more training\n",
    "- Raw text is harder to learn than instruction pairs\n",
    "- Need to update model's internal representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,  # Enable packing for efficiency\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=4,  # Can use larger batch for pretraining\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,  # Multiple epochs for domain adaptation\n",
    "        learning_rate=1e-4,  # Higher learning rate for pretraining\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",  # Cosine decay for pretraining\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs_pretraining\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Trainer configured for CONTINUED PRETRAINING!\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Batch size: 4\")\n",
    "print(f\"  Gradient accumulation: 4\")\n",
    "print(f\"  Effective batch size: 16\")\n",
    "print(f\"  Epochs: 3\")\n",
    "print(f\"  Learning rate: 1e-4 (higher for pretraining)\")\n",
    "print(f\"  Packing: Enabled\")\n",
    "print(f\"  Total steps: ~{len(dataset) * 3 // 16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849ef76",
   "metadata": {},
   "source": [
    "## Step 9: Start Continued Pretraining!\n",
    "\n",
    "This will adapt the model to Python programming domain.\n",
    "Training time: ~15-30 minutes on T4 GPU (longer than finetuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72200165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nStarting continued pretraining on Python code...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show GPU memory after training\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "training_time = trainer_stats.metrics['train_runtime']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Continued pretraining completed successfully!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Memory used for training = {used_memory_for_training} GB.\")\n",
    "print(f\"Percentage of max memory used = {used_percentage}%\")\n",
    "print(f\"Training time = {training_time:.2f} seconds ({training_time/60:.1f} minutes)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b5324",
   "metadata": {},
   "source": [
    "## Step 10: Test the Domain-Adapted Model\n",
    "\n",
    "Let's test if the model improved at Python coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompts - Python coding tasks\n",
    "test_prompts = [\n",
    "    \"def fibonacci(n):\",\n",
    "    \"def merge_sort(arr):\",\n",
    "    \"class BinaryTree:\",\n",
    "    \"# Function to reverse a string\\ndef reverse_string(s):\",\n",
    "]\n",
    "\n",
    "print(\"Testing the domain-adapted model on Python code...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test {i}: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=150,\n",
    "        use_cache=True,\n",
    "        temperature=0.3,  # Lower temperature for code\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bce197",
   "metadata": {},
   "source": [
    "## Step 11: Compare Before vs After\n",
    "\n",
    "### What Changed?\n",
    "- **Before**: Model knows basic Python syntax\n",
    "- **After**: Model has deeper knowledge of Python patterns, libraries, and idioms\n",
    "\n",
    "### How to Verify Improvement:\n",
    "1. **Perplexity**: Lower perplexity on Python code\n",
    "2. **Code Quality**: Better function implementations\n",
    "3. **Domain Knowledge**: Uses appropriate libraries and patterns\n",
    "4. **Consistency**: More coherent code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a2379",
   "metadata": {},
   "source": [
    "## Step 12: Save the Domain-Adapted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"smollm2_135m_python_adapted\")\n",
    "tokenizer.save_pretrained(\"smollm2_135m_python_adapted\")\n",
    "\n",
    "print(\"Domain-adapted model saved to 'smollm2_135m_python_adapted/'\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use this as a base model for Python-specific finetuning\")\n",
    "print(\"2. Continue pretraining on more Python code\")\n",
    "print(\"3. Finetune on instruction-following tasks (Colab 1 or 2)\")\n",
    "print(\"4. Apply DPO for preference alignment (Colab 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b5c2e0",
   "metadata": {},
   "source": [
    "## Step 13: (Optional) Further Finetuning\n",
    "\n",
    "### Typical Workflow:\n",
    "```\n",
    "1. Continued Pretraining (this notebook)\n",
    "   ‚Üì\n",
    "2. Instruction Finetuning (Colab 1 or 2)\n",
    "   ‚Üì\n",
    "3. Preference Alignment (Colab 3 - DPO)\n",
    "   ‚Üì\n",
    "4. Reasoning Enhancement (Colab 4 - GRPO)\n",
    "```\n",
    "\n",
    "### Why This Order?\n",
    "- **Pretraining**: Builds domain knowledge foundation\n",
    "- **Finetuning**: Teaches instruction following\n",
    "- **DPO**: Aligns with human preferences\n",
    "- **GRPO**: Enhances reasoning abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d2b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now load this model for further finetuning:\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=\"smollm2_135m_python_adapted\",\n",
    "#     max_seq_length=2048,\n",
    "#     dtype=None,\n",
    "#     load_in_4bit=True,\n",
    "# )\n",
    "\n",
    "print(\"Ready for next stage of training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65c4d7",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We Did:\n",
    "1. ‚úÖ Loaded SmolLM2 135M model\n",
    "2. ‚úÖ Configured LoRA for continued pretraining\n",
    "3. ‚úÖ Loaded CodeSearchNet Python dataset\n",
    "4. ‚úÖ Formatted raw code text (no instruction template)\n",
    "5. ‚úÖ Trained on domain-specific data (3 epochs)\n",
    "6. ‚úÖ Tested Python code generation\n",
    "7. ‚úÖ Saved domain-adapted model\n",
    "\n",
    "### Continued Pretraining vs Other Methods:\n",
    "| Method | Data Format | Learning Rate | Epochs | Purpose |\n",
    "|--------|-------------|---------------|--------|----------|\n",
    "| **Continued Pretraining** | Raw text | 1e-4 (high) | 3-5 | Domain knowledge |\n",
    "| **Full Finetuning** | Instruction pairs | 2e-4 | 1-3 | Instruction following |\n",
    "| **LoRA Finetuning** | Instruction pairs | 2e-4 | 1-3 | Efficient instruction following |\n",
    "| **DPO** | Preference pairs | 5e-5 (low) | 1-2 | Alignment |\n",
    "| **GRPO** | Prompts only | 5e-6 (lower) | Many | Reasoning |\n",
    "\n",
    "### When to Use Continued Pretraining:\n",
    "1. **New Domain**: Medical, legal, scientific domains\n",
    "2. **New Language**: Adding language support\n",
    "3. **Specialized Knowledge**: Finance, chemistry, etc.\n",
    "4. **Recent Data**: Updating with new information\n",
    "5. **Code Languages**: New programming languages\n",
    "\n",
    "### Dataset Requirements:\n",
    "```python\n",
    "# Good for continued pretraining:\n",
    "- Large corpus of domain text (100K+ examples)\n",
    "- High quality, well-formatted text\n",
    "- Representative of target domain\n",
    "- Raw text, not Q&A pairs\n",
    "```\n",
    "\n",
    "### Tips for Better Results:\n",
    "1. **More Data**: Use 50K-1M examples for production\n",
    "2. **Multiple Epochs**: Train for 3-5 epochs\n",
    "3. **Higher Rank**: Use r=32-64 for LoRA\n",
    "4. **Enable Packing**: Improves efficiency\n",
    "5. **Monitor Loss**: Watch for convergence\n",
    "6. **Quality Over Quantity**: Clean, relevant data is key\n",
    "\n",
    "### Comparison with Other Colabs:\n",
    "- **vs Colab 1 (Full Finetuning)**: Uses raw text, not instructions\n",
    "- **vs Colab 2 (LoRA)**: Higher rank, more epochs, raw text\n",
    "- **vs Colab 3 (DPO)**: No preference pairs, just domain text\n",
    "- **vs Colab 4 (GRPO)**: No reward function, just language modeling\n",
    "\n",
    "### Next Steps:\n",
    "1. Try other domains (medical, legal, etc.)\n",
    "2. Increase dataset size for better results\n",
    "3. Combine with instruction finetuning\n",
    "4. Apply DPO for alignment\n",
    "5. Use GRPO for reasoning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849a0f6",
   "metadata": {},
   "source": [
    "# Colab 5: Continued Pretraining\n",
    "## Teaching an LLM New Knowledge and Languages\n",
    "\n",
    "This notebook demonstrates **Continued Pretraining (CPT)** - teaching an existing LLM completely new knowledge, languages, or domains.\n",
    "\n",
    "### What is Continued Pretraining?\n",
    "- **CPT**: Further pretraining on domain-specific or new data\n",
    "- **Unlike Finetuning**: Learns new facts, vocabulary, and patterns\n",
    "- **Expands Knowledge**: Doesn't just format existing knowledge\n",
    "- **Use Cases**: New languages, specialized domains, recent events\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Training Type | Purpose | Data Format | Example |\n",
    "|--------------|---------|-------------|----------|\n",
    "| **Pretraining** | Learn language | Raw text | Wikipedia |\n",
    "| **Continued Pretraining** | Learn new domain | Domain text | Medical papers |\n",
    "| **Finetuning (SFT)** | Learn format | Q&A pairs | Instruction pairs |\n",
    "| **Alignment (DPO)** | Learn preferences | Chosen/Rejected | Human feedback |\n",
    "\n",
    "### What You'll Learn:\n",
    "1. How to prepare raw text for continued pretraining\n",
    "2. Training settings for learning new knowledge\n",
    "3. Teaching a model a new language (e.g., code, domain jargon)\n",
    "4. Evaluation of new capabilities\n",
    "\n",
    "### Example Use Cases:\n",
    "- üè• **Medical LLM**: Train on medical literature\n",
    "- ‚öñÔ∏è **Legal LLM**: Train on legal documents\n",
    "- üíª **Code LLM**: Train on code repositories\n",
    "- üåç **Multilingual**: Add new languages\n",
    "- üì∞ **Current Events**: Update with recent information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684fd17",
   "metadata": {},
   "source": [
    "## Step 1: Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6660c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install --upgrade datasets transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1755759",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b66866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46543dd1",
   "metadata": {},
   "source": [
    "## Step 3: Load Base Model\n",
    "\n",
    "We'll use a small model and teach it new knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Using SmolLM2 135M for demonstration\n",
    "model_name = \"unsloth/SmolLM2-135M-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(\"We'll teach this model new domain knowledge!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381cd4fe",
   "metadata": {},
   "source": [
    "## Step 4: Add LoRA Adapters\n",
    "\n",
    "Even for continued pretraining, LoRA is efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbc8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # Higher rank for learning new knowledge\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,  # Small dropout for regularization\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters configured!\")\n",
    "print(\"Using r=64 for maximum learning capacity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec777ef8",
   "metadata": {},
   "source": [
    "## Step 5: Choose Your Domain\n",
    "\n",
    "### Option 1: Python Programming\n",
    "Teach the model Python code patterns\n",
    "\n",
    "### Option 2: Medical/Scientific Text\n",
    "Teach medical or scientific knowledge\n",
    "\n",
    "### Option 3: Custom Domain\n",
    "Your own specialized knowledge\n",
    "\n",
    "For this demo, we'll use **Python code** as our new domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49eda1",
   "metadata": {},
   "source": [
    "## Step 6: Load Domain-Specific Data\n",
    "\n",
    "### Data Format for Continued Pretraining:\n",
    "- **Raw text** (not Q&A pairs!)\n",
    "- **Large corpus** of domain knowledge\n",
    "- **Natural format** as it appears in the domain\n",
    "\n",
    "We'll use a Python code dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6677eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load Python code dataset\n",
    "print(\"Loading Python code dataset for continued pretraining...\")\n",
    "\n",
    "# Using a subset of the CodeSearchNet Python dataset\n",
    "dataset = load_dataset(\n",
    "    \"code_search_net\",\n",
    "    \"python\",\n",
    "    split=\"train[:5000]\"  # Using 5000 samples\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} code samples\")\n",
    "print(f\"\\nSample code:\")\n",
    "print(dataset[0]['whole_func_string'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04158e2f",
   "metadata": {},
   "source": [
    "## Step 7: Prepare Text for Pretraining\n",
    "\n",
    "### Key Differences from Finetuning:\n",
    "1. **No templates**: Just raw text\n",
    "2. **Longer sequences**: More context\n",
    "3. **Next token prediction**: Standard language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_pretraining(examples):\n",
    "    \"\"\"\n",
    "    Prepare raw text for continued pretraining.\n",
    "    For code: Include function with docstring\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for func_string, docstring in zip(examples['whole_func_string'], examples['func_documentation_string']):\n",
    "        # Format: docstring + code (natural format)\n",
    "        text = f\"# {docstring}\\n{func_string}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(\n",
    "    prepare_for_pretraining,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Dataset prepared for continued pretraining!\")\n",
    "print(f\"\\nExample formatted text:\")\n",
    "print(dataset[0]['text'][:400] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecba84a",
   "metadata": {},
   "source": [
    "## Alternative: Create Custom Domain Dataset\n",
    "\n",
    "You can also create your own domain-specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom medical/scientific text\n",
    "# Uncomment to use:\n",
    "\n",
    "# custom_texts = [\n",
    "#     \"\"\"Machine learning is a subset of artificial intelligence that focuses on \n",
    "#     building systems that can learn from data. Neural networks are composed of \n",
    "#     interconnected nodes that process information...\"\"\",\n",
    "#     \n",
    "#     \"\"\"Deep learning uses multiple layers of neural networks to progressively \n",
    "#     extract higher-level features from raw input. This hierarchical approach...\"\"\",\n",
    "#     \n",
    "#     # Add more domain texts...\n",
    "# ]\n",
    "# \n",
    "# custom_dataset = Dataset.from_dict({\"text\": custom_texts})\n",
    "# dataset = custom_dataset\n",
    "\n",
    "print(\"To use custom domain text, uncomment and modify the cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1edee3",
   "metadata": {},
   "source": [
    "## Step 8: Test Model BEFORE Continued Pretraining\n",
    "\n",
    "Let's see what the model knows about Python before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"Testing model BEFORE continued pretraining on Python code...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_prompt = \"\"\"Write a Python function to calculate the factorial of a number:\n",
    "\n",
    "def factorial(n):\"\"\"\n",
    "\n",
    "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\nModel's completion (BEFORE training):\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "outputs_before = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e36bd",
   "metadata": {},
   "source": [
    "## Step 9: Configure Training for Continued Pretraining\n",
    "\n",
    "### Important Settings:\n",
    "- **Higher learning rate**: 5e-5 to 2e-4 (learning new knowledge)\n",
    "- **More epochs/steps**: Need more exposure to new domain\n",
    "- **Longer sequences**: More context helps learning\n",
    "- **No response template**: Just raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,  # Pack sequences for efficiency\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,  # More epochs for deeper learning\n",
    "        learning_rate=1e-4,  # Higher learning rate for new knowledge\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",  # Cosine scheduler for CPT\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Trainer configured for continued pretraining!\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Training mode: CONTINUED PRETRAINING\")\n",
    "print(f\"  Learning rate: 1e-4 (higher for new knowledge)\")\n",
    "print(f\"  Epochs: 3\")\n",
    "print(f\"  Packing: True (efficiency)\")\n",
    "print(f\"  Scheduler: Cosine\")\n",
    "print(f\"  Domain: Python Programming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae74248",
   "metadata": {},
   "source": [
    "## Step 10: Start Continued Pretraining!\n",
    "\n",
    "The model will learn Python patterns, syntax, and idioms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38959af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nStarting continued pretraining on Python code...\")\n",
    "print(\"The model is learning Python! üêç\\n\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show statistics\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "training_time = trainer_stats.metrics['train_runtime']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Continued Pretraining completed successfully!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Memory used for training = {used_memory_for_training} GB.\")\n",
    "print(f\"Percentage of max memory used = {used_percentage}%\")\n",
    "print(f\"Training time = {training_time:.2f} seconds\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3489f",
   "metadata": {},
   "source": [
    "## Step 11: Test Model AFTER Continued Pretraining\n",
    "\n",
    "Let's see if the model improved at writing Python code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"Testing model AFTER continued pretraining on Python code...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_prompts = [\n",
    "    \"\"\"Write a Python function to calculate the factorial of a number:\n",
    "\n",
    "def factorial(n):\"\"\",\n",
    "    \n",
    "    \"\"\"Write a Python function to check if a number is prime:\n",
    "\n",
    "def is_prime(n):\"\"\",\n",
    "    \n",
    "    \"\"\"Write a Python function to reverse a string:\n",
    "\n",
    "def reverse_string(s):\"\"\"\n",
    "]\n",
    "\n",
    "for i, test_prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(\"Prompt:\")\n",
    "    print(test_prompt)\n",
    "    print(\"\\nModel's completion (AFTER training):\")\n",
    "    \n",
    "    inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf220d20",
   "metadata": {},
   "source": [
    "## Step 12: Compare Before vs After\n",
    "\n",
    "### What Changed?\n",
    "After continued pretraining, you should see:\n",
    "- ‚úÖ Better Python syntax\n",
    "- ‚úÖ More idiomatic code\n",
    "- ‚úÖ Proper function structure\n",
    "- ‚úÖ Better understanding of Python patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüí° Key Improvements After Continued Pretraining:\")\n",
    "print(\"=\"*60)\n",
    "print(\"BEFORE: Generic or incorrect code\")\n",
    "print(\"AFTER: Proper Python with correct patterns\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nWhat the model learned:\")\n",
    "print(\"  ‚Ä¢ Python syntax and structure\")\n",
    "print(\"  ‚Ä¢ Common programming patterns\")\n",
    "print(\"  ‚Ä¢ Idiomatic Python style\")\n",
    "print(\"  ‚Ä¢ Function documentation\")\n",
    "print(\"  ‚Ä¢ Code organization\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0660c7",
   "metadata": {},
   "source": [
    "## Step 13: Save the Domain-Adapted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"smollm2_python_pretrained\")\n",
    "tokenizer.save_pretrained(\"smollm2_python_pretrained\")\n",
    "\n",
    "print(\"Domain-adapted model saved to 'smollm2_python_pretrained/'\")\n",
    "print(\"\\nThis model now has deep knowledge of Python programming!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c45167",
   "metadata": {},
   "source": [
    "## Step 14: (Optional) Further Finetuning\n",
    "\n",
    "After continued pretraining, you can do instruction finetuning for even better results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ Recommended Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Continued Pretraining (Done!) ‚Üí Learn Python\")\n",
    "print(\"2. Instruction Finetuning (Next) ‚Üí Learn to follow instructions\")\n",
    "print(\"3. DPO Alignment (Optional) ‚Üí Align with preferences\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis creates a powerful domain-specific model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b434b",
   "metadata": {},
   "source": [
    "## Alternative Domains to Explore\n",
    "\n",
    "You can adapt this notebook for any domain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüåü Other Domains to Try:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. üè• Medical:\")\n",
    "print(\"   Dataset: PubMed abstracts, medical papers\")\n",
    "print(\"   Use: Medical diagnosis, literature review\")\n",
    "\n",
    "print(\"\\n2. ‚öñÔ∏è Legal:\")\n",
    "print(\"   Dataset: Legal documents, case law\")\n",
    "print(\"   Use: Legal research, document analysis\")\n",
    "\n",
    "print(\"\\n3. üíª Code (Other Languages):\")\n",
    "print(\"   Dataset: JavaScript, Java, C++ repositories\")\n",
    "print(\"   Use: Multi-language code generation\")\n",
    "\n",
    "print(\"\\n4. üåç New Natural Language:\")\n",
    "print(\"   Dataset: Text in target language\")\n",
    "print(\"   Use: Multilingual capabilities\")\n",
    "\n",
    "print(\"\\n5. üî¨ Scientific:\")\n",
    "print(\"   Dataset: Research papers, arXiv\")\n",
    "print(\"   Use: Scientific reasoning, literature\")\n",
    "\n",
    "print(\"\\n6. üì∞ News/Current Events:\")\n",
    "print(\"   Dataset: Recent news articles\")\n",
    "print(\"   Use: Up-to-date knowledge\")\n",
    "\n",
    "print(\"\\n7. üéÆ Gaming:\")\n",
    "print(\"   Dataset: Game wikis, strategy guides\")\n",
    "print(\"   Use: Game AI, strategy assistant\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf58e3",
   "metadata": {},
   "source": [
    "## Summary & Key Concepts\n",
    "\n",
    "### What is Continued Pretraining?\n",
    "**Continued Pretraining (CPT)** is the process of:\n",
    "- Taking an existing pretrained model\n",
    "- Training it further on domain-specific raw text\n",
    "- Teaching it new vocabulary, facts, and patterns\n",
    "- Expanding its knowledge beyond original training\n",
    "\n",
    "### Training Pipeline:\n",
    "```\n",
    "1. Base Model (e.g., SmolLM2)\n",
    "   ‚Üì\n",
    "2. Continued Pretraining (Domain Knowledge)\n",
    "   ‚Üì\n",
    "3. Instruction Finetuning (Format)\n",
    "   ‚Üì\n",
    "4. Alignment (DPO/RLHF)\n",
    "   ‚Üì\n",
    "5. Domain Expert Model!\n",
    "```\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Aspect | Pretraining | Continued Pretraining | Finetuning |\n",
    "|--------|-------------|----------------------|------------|\n",
    "| **Data** | General text | Domain text | Q&A pairs |\n",
    "| **Format** | Raw text | Raw text | Structured |\n",
    "| **Goal** | Learn language | Learn domain | Learn format |\n",
    "| **Scale** | Trillions | Millions-Billions | Thousands |\n",
    "| **LR** | 1e-4 to 3e-4 | 5e-5 to 2e-4 | 1e-5 to 5e-5 |\n",
    "\n",
    "### Data Preparation:\n",
    "\n",
    "**For Continued Pretraining**:\n",
    "```python\n",
    "# Just raw text, no templates!\n",
    "texts = [\n",
    "    \"Python is a high-level programming language...\",\n",
    "    \"Machine learning involves training models...\",\n",
    "    # More raw domain text\n",
    "]\n",
    "```\n",
    "\n",
    "**NOT like finetuning**:\n",
    "```python\n",
    "# Don't use this format for CPT:\n",
    "# {\"instruction\": \"...\", \"output\": \"...\"}\n",
    "```\n",
    "\n",
    "### Training Settings:\n",
    "\n",
    "**Learning Rate**: \n",
    "- Higher than finetuning: 5e-5 to 2e-4\n",
    "- Lower than initial pretraining: not 3e-4\n",
    "\n",
    "**Epochs/Steps**:\n",
    "- More than finetuning: 3-10 epochs\n",
    "- Need repeated exposure to new knowledge\n",
    "\n",
    "**Sequence Length**:\n",
    "- Longer is better: 2048-4096 tokens\n",
    "- More context helps learning\n",
    "\n",
    "**Batch Size**:\n",
    "- Larger if possible: 8-64 effective batch size\n",
    "- Stable gradient estimates\n",
    "\n",
    "**Scheduler**:\n",
    "- Cosine decay works well\n",
    "- Gradual learning rate reduction\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "**1. Domain Adaptation**:\n",
    "- Medical, legal, financial\n",
    "- Specialized vocabulary\n",
    "- Domain-specific patterns\n",
    "\n",
    "**2. Language Addition**:\n",
    "- Add new natural languages\n",
    "- Programming languages\n",
    "- Domain-specific jargon\n",
    "\n",
    "**3. Knowledge Updates**:\n",
    "- Recent events\n",
    "- New discoveries\n",
    "- Updated information\n",
    "\n",
    "**4. Private Data**:\n",
    "- Company documents\n",
    "- Internal knowledge bases\n",
    "- Proprietary information\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "‚úÖ **Deep Knowledge**: True understanding, not just formatting\n",
    "‚úÖ **Vocabulary**: Learns new terms and concepts\n",
    "‚úÖ **Patterns**: Understands domain-specific structures\n",
    "‚úÖ **Generalization**: Better than just memorizing examples\n",
    "\n",
    "### When to Use CPT:\n",
    "\n",
    "**Use CPT when**:\n",
    "- Model lacks domain knowledge\n",
    "- Need deep expertise\n",
    "- Have large corpus of domain text\n",
    "- Want to add new language\n",
    "\n",
    "**Skip CPT when**:\n",
    "- Model already knows the domain\n",
    "- Only need formatting\n",
    "- Limited domain data\n",
    "- Quick task-specific adaptation\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "**1. Data Quality**:\n",
    "- High-quality domain text\n",
    "- Clean and well-formatted\n",
    "- Representative of domain\n",
    "\n",
    "**2. Data Quantity**:\n",
    "- At least 10MB of text\n",
    "- Ideally 100MB-1GB+\n",
    "- More data = better results\n",
    "\n",
    "**3. Progressive Training**:\n",
    "```\n",
    "Stage 1: Continued Pretraining (raw text)\n",
    "Stage 2: Instruction Finetuning (Q&A)\n",
    "Stage 3: Alignment (preferences)\n",
    "```\n",
    "\n",
    "**4. Evaluation**:\n",
    "- Test on domain tasks\n",
    "- Compare before/after\n",
    "- Use domain experts\n",
    "\n",
    "**5. Monitoring**:\n",
    "- Watch for catastrophic forgetting\n",
    "- Test general capabilities\n",
    "- Balance old and new knowledge\n",
    "\n",
    "### Common Mistakes:\n",
    "\n",
    "‚ùå **Using templates**: CPT needs raw text\n",
    "‚ùå **Too little data**: Need substantial corpus\n",
    "‚ùå **Too low LR**: Won't learn new knowledge\n",
    "‚ùå **Too few epochs**: Need multiple passes\n",
    "‚ùå **Skipping validation**: Test domain knowledge\n",
    "\n",
    "### Real-World Examples:\n",
    "\n",
    "**BloombergGPT**:\n",
    "- Continued pretraining on financial data\n",
    "- 363B tokens of financial text\n",
    "- Outperforms general models on finance\n",
    "\n",
    "**BioGPT**:\n",
    "- Continued pretraining on PubMed\n",
    "- 15M biomedical papers\n",
    "- State-of-art on biomedical NLP\n",
    "\n",
    "**CodeLlama**:\n",
    "- Continued pretraining on code\n",
    "- 500B tokens of code\n",
    "- Excellent code generation\n",
    "\n",
    "### Measuring Success:\n",
    "\n",
    "**Qualitative**:\n",
    "- Better domain terminology\n",
    "- More accurate facts\n",
    "- Appropriate style\n",
    "\n",
    "**Quantitative**:\n",
    "- Domain-specific benchmarks\n",
    "- Perplexity on domain text\n",
    "- Task accuracy\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Gather Domain Data**:\n",
    "   - Collect 100MB+ of quality text\n",
    "   - Clean and preprocess\n",
    "\n",
    "2. **Run CPT**:\n",
    "   - Use this notebook as template\n",
    "   - Train for 3-10 epochs\n",
    "   - Monitor loss\n",
    "\n",
    "3. **Evaluate**:\n",
    "   - Test domain knowledge\n",
    "   - Compare to base model\n",
    "\n",
    "4. **Instruction Tune**:\n",
    "   - Use Colab 1 or 2\n",
    "   - Teach instruction following\n",
    "\n",
    "5. **Deploy**:\n",
    "   - Export to GGUF\n",
    "   - Use with Ollama\n",
    "   - Production ready!\n",
    "\n",
    "### Resources:\n",
    "- [Unsloth CPT Guide](https://docs.unsloth.ai/basics/continued-pretraining)\n",
    "- [Domain Adaptation Papers](https://arxiv.org/)\n",
    "- [Code Datasets](https://huggingface.co/datasets?task_categories=code)\n",
    "- [Domain-Specific Datasets](https://huggingface.co/datasets)\n",
    "\n",
    "### Conclusion:\n",
    "Continued Pretraining is essential for:\n",
    "- ‚úÖ Domain expertise\n",
    "- ‚úÖ New language learning\n",
    "- ‚úÖ Knowledge expansion\n",
    "- ‚úÖ Specialized applications\n",
    "\n",
    "**CPT + Finetuning + Alignment = Domain Expert AI! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68ce7e",
   "metadata": {},
   "source": [
    "# Colab 5 ‚Äî Continued Pretraining (New Language / Domain)\n",
    "\n",
    "This notebook shows the steps to perform continued pretraining on a plain-text corpus (for example to teach a model a new language or domain). Continued pretraining typically uses a language-modeling objective over large text corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dc9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install --upgrade pip\n",
    "!pip install unslothai datasets transformers accelerate --quiet\n",
    "print('Upload your continued-pretraining corpus as plain text (datasets/continued_pretraining.txt)')\n",
    ": \n",
    ",\n",
    ": {\n",
    ": \n",
    "},\n",
    ": [\n",
    ",\n",
    "1\n",
    ",\n",
    "2\n",
    ",\n",
    "3\n",
    " \n",
    ": {\n",
    ": {\n",
    ": \n",
    ",\n",
    ": \n",
    "3\n",
    "},\n",
    ": {\n",
    ": \n",
    "}},\n",
    ": 4,\n",
    ": 5"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
