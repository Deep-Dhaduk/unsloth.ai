{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d6aa23",
   "metadata": {},
   "source": [
    "# Colab 1: Full Finetuning with SmolLM2 135M\n",
    "## Complete Fine-tuning (Not LoRA) using Unsloth\n",
    "\n",
    "This notebook demonstrates **full finetuning** of the SmolLM2 135M parameter model using unsloth.ai.\n",
    "\n",
    "### What is Full Finetuning?\n",
    "- **Full Finetuning**: Updates ALL model parameters during training\n",
    "- **More Memory Intensive**: Requires more VRAM than LoRA\n",
    "- **Better Performance**: Can achieve better results for specific tasks\n",
    "- **Use Case**: Chat/conversation fine-tuning\n",
    "\n",
    "### Key Points:\n",
    "- Model: `unsloth/SmolLM2-135M-Instruct` (smallest model for faster training)\n",
    "- Dataset: We'll use a chat/instruction dataset\n",
    "- Method: Full parameter training (not LoRA)\n",
    "- Template: Chat template for instruction following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642ff02",
   "metadata": {},
   "source": [
    "## Step 1: Install Unsloth\n",
    "Unsloth provides 2x faster training and 80% less memory usage compared to standard approaches.\n",
    "\n",
    "**Important**: We need to use `datasets==4.3.0` to avoid recursion errors with Unsloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d151f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install unsloth with all dependencies\n",
    "!pip install unsloth\n",
    "# Install compatible version of datasets (4.3.0 to avoid recursion errors)\n",
    "!pip install datasets==4.3.0\n",
    "# Also install other useful libraries\n",
    "!pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86248718",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3119dbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth cannot find any torch accelerator? You need a GPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\unsloth\\__init__.py:80\u001b[39m\n\u001b[32m     68\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     69\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please update Unsloth and Unsloth-Zoo to the latest version!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDo this via `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         )\n\u001b[32m     72\u001b[39m         \u001b[38;5;66;03m# if os.environ.get(\"UNSLOTH_DISABLE_AUTO_UPDATES\", \"0\") == \"0\":\u001b[39;00m\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m#         os.system(\"pip install --upgrade --no-cache-dir --no-deps unsloth_zoo\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m         \u001b[38;5;66;03m#         except:\u001b[39;00m\n\u001b[32m     79\u001b[39m         \u001b[38;5;66;03m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     83\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo` then retry!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\unsloth_zoo\\__init__.py:136\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m logging, torchao_logger, HideLoggingMessage\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Get device types and other variables\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdevice_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    137\u001b[39m     is_hip,\n\u001b[32m    138\u001b[39m     get_device_type,\n\u001b[32m    139\u001b[39m     DEVICE_TYPE,\n\u001b[32m    140\u001b[39m     DEVICE_TYPE_TORCH,\n\u001b[32m    141\u001b[39m     DEVICE_COUNT,\n\u001b[32m    142\u001b[39m     ALLOW_PREQUANTIZED_MODELS,\n\u001b[32m    143\u001b[39m )\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Torch 2.9 removed PYTORCH_HIP_ALLOC_CONF and PYTORCH_CUDA_ALLOC_CONF\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m major_torch == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m minor_torch >= \u001b[32m9\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\unsloth_zoo\\device_type.py:56\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth currently only works on NVIDIA, AMD and Intel GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m DEVICE_TYPE : \u001b[38;5;28mstr\u001b[39m = \u001b[43mget_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# HIP fails for autocast and other torch functions. Use CUDA instead\u001b[39;00m\n\u001b[32m     58\u001b[39m DEVICE_TYPE_TORCH = DEVICE_TYPE\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\unsloth_zoo\\device_type.py:46\u001b[39m, in \u001b[36mget_device_type\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[33m\"\u001b[39m\u001b[33maccelerator\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.accelerator.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth cannot find any torch accelerator? You need a GPU.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m     accelerator = \u001b[38;5;28mstr\u001b[39m(torch.accelerator.current_accelerator())\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m accelerator \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhip\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mNotImplementedError\u001b[39m: Unsloth cannot find any torch accelerator? You need a GPU."
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632362c",
   "metadata": {},
   "source": [
    "## Step 3: Configure Model Parameters\n",
    "\n",
    "### Important Parameters Explained:\n",
    "- **max_seq_length**: Maximum sequence length (2048 tokens)\n",
    "- **dtype**: Data type (None = auto-detect, can be float16 or bfloat16)\n",
    "- **load_in_4bit**: Use 4-bit quantization to save memory (True)\n",
    "\n",
    "### Note about Full Finetuning:\n",
    "- **r, lora_alpha, lora_dropout**: NOT USED in full finetuning (only for LoRA)\n",
    "- Full finetuning doesn't use `get_peft_model()` at all\n",
    "- We'll enable gradient checkpointing instead for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "max_seq_length = 2048  # Can be increased for longer sequences\n",
    "dtype = None  # Auto-detect. Can use Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage\n",
    "\n",
    "# Model name - using SmolLM2 135M (smallest for fastest training)\n",
    "model_name = \"unsloth/SmolLM2-135M-Instruct\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(f\"  Max Sequence Length: {max_seq_length}\")\n",
    "print(f\"  4-bit Quantization: {load_in_4bit}\")\n",
    "print(f\"  Training Mode: FULL FINETUNING (not LoRA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd709ebd",
   "metadata": {},
   "source": [
    "## Step 4: Load the Model and Tokenizer\n",
    "\n",
    "We're loading the SmolLM2 135M model with 4-bit quantization for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d41110",
   "metadata": {},
   "source": [
    "## Step 5: Configure for FULL FINETUNING\n",
    "\n",
    "### CRITICAL: For Full Finetuning, Skip get_peft_model()!\n",
    "\n",
    "**What happens here:**\n",
    "- We do **NOT** call `get_peft_model()` - that's only for LoRA!\n",
    "- For full finetuning, we use the model directly\n",
    "- We enable gradient checkpointing for memory efficiency\n",
    "- All 135M parameters will be updated during training\n",
    "\n",
    "**Difference:**\n",
    "- **LoRA**: Call `get_peft_model()` with `r=16` or higher\n",
    "- **Full Finetuning**: Skip `get_peft_model()`, use model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For FULL finetuning, we DON'T call get_peft_model()\n",
    "# We use the model directly and enable gradient checkpointing for memory efficiency\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"Model configured for FULL FINETUNING!\")\n",
    "print(\"All 135M model parameters will be updated during training.\")\n",
    "print(\"Gradient checkpointing enabled for memory efficiency.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fc2a2",
   "metadata": {},
   "source": [
    "## Step 6: Prepare the Dataset\n",
    "\n",
    "### Dataset Format:\n",
    "We'll use a simple instruction-following dataset. The format is:\n",
    "```\n",
    "Input: <instruction>\n",
    "Output: <response>\n",
    "```\n",
    "\n",
    "### Chat Template:\n",
    "The model expects a specific chat format:\n",
    "```\n",
    "<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_message}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant_response}<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a simple instruction dataset\n",
    "# Using a subset of the Alpaca dataset for chat/instruction following\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:1000]\")  # Using 1000 samples for quick training\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"\\nSample data point:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da5bf4",
   "metadata": {},
   "source": [
    "## Step 7: Format Dataset with Chat Template\n",
    "\n",
    "We need to convert the dataset into the proper chat format that SmolLM2 expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c4e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chat template formatting function\n",
    "chat_template = \"\"\"<|im_start|>system\n",
    "You are a helpful AI assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}{input}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{output}<|im_end|>\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # Combine instruction and input\n",
    "        input_section = f\"\\n\\nInput: {input_text}\" if input_text else \"\"\n",
    "        \n",
    "        # Format with chat template\n",
    "        text = chat_template.format(\n",
    "            instruction=instruction,\n",
    "            input=input_section,\n",
    "            output=output\n",
    "        ) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting to dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(\"Dataset formatted successfully!\")\n",
    "print(f\"\\nExample formatted text:\")\n",
    "print(dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4beb3",
   "metadata": {},
   "source": [
    "## Step 8: Configure Training Arguments\n",
    "\n",
    "### Training Parameters Explained:\n",
    "- **per_device_train_batch_size**: Number of samples per batch (2 for memory efficiency)\n",
    "- **gradient_accumulation_steps**: Accumulate gradients over 4 steps (effective batch size = 2*4=8)\n",
    "- **warmup_steps**: Gradually increase learning rate over 5 steps\n",
    "- **max_steps**: Total training steps (60 for quick demo, increase for better results)\n",
    "- **learning_rate**: 2e-4 for full finetuning (higher than LoRA)\n",
    "- **fp16/bf16**: Use mixed precision training\n",
    "- **logging_steps**: Log every step\n",
    "- **optim**: Use AdamW 8-bit optimizer to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f58a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,  # Increase for better results (e.g., 500-1000)\n",
    "        learning_rate=2e-4,  # Higher learning rate for full finetuning\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",  # Disable wandb logging\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Trainer configured successfully!\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Batch size: 2\")\n",
    "print(f\"  Gradient accumulation: 4\")\n",
    "print(f\"  Effective batch size: 8\")\n",
    "print(f\"  Max steps: 60\")\n",
    "print(f\"  Learning rate: 2e-4\")\n",
    "print(f\"  Training mode: FULL FINETUNING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b01f2",
   "metadata": {},
   "source": [
    "## Step 9: Start Training!\n",
    "\n",
    "This will train ALL 135M parameters of the model.\n",
    "Training time: ~5-10 minutes on a T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9657247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nStarting full finetuning...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show GPU memory after training\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "training_time = trainer_stats.metrics['train_runtime']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training completed successfully!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Memory used for training = {used_memory_for_training} GB.\")\n",
    "print(f\"Percentage of max memory used = {used_percentage}%\")\n",
    "print(f\"Training time = {training_time:.2f} seconds\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17954882",
   "metadata": {},
   "source": [
    "## Step 10: Test the Fine-tuned Model (Inference)\n",
    "\n",
    "Let's test our fully fine-tuned model with some sample prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622fcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Explain what machine learning is in simple terms.\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"What are the benefits of exercise?\"\n",
    "]\n",
    "\n",
    "print(\"Testing the fine-tuned model...\\n\")\n",
    "\n",
    "for i, instruction in enumerate(test_prompts, 1):\n",
    "    # Format input with chat template\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "You are a helpful AI assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test {i}: {instruction}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cc5da",
   "metadata": {},
   "source": [
    "## Step 11: Save the Model\n",
    "\n",
    "### Save Options:\n",
    "1. **Local saving**: Save to disk\n",
    "2. **GGUF format**: For Ollama/llama.cpp\n",
    "3. **Push to Hugging Face**: Share with the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542efbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally\n",
    "model.save_pretrained(\"smollm2_135m_full_finetuned\")\n",
    "tokenizer.save_pretrained(\"smollm2_135m_full_finetuned\")\n",
    "\n",
    "print(\"Model saved locally to 'smollm2_135m_full_finetuned/'\")\n",
    "print(\"\\nYou can now:\")\n",
    "print(\"1. Use it for inference\")\n",
    "print(\"2. Continue training from this checkpoint\")\n",
    "print(\"3. Export to GGUF for Ollama\")\n",
    "print(\"4. Push to Hugging Face Hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1fad5",
   "metadata": {},
   "source": [
    "## Step 12: (Optional) Save in GGUF Format for Ollama\n",
    "\n",
    "GGUF format allows you to run the model locally with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4845611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in different quantization formats\n",
    "# Uncomment the quantization method you want:\n",
    "\n",
    "# Q4_K_M - recommended for most use cases (good balance)\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "# Q8_0 - high quality, larger file size\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q8_0\")\n",
    "\n",
    "# F16 - full precision, largest file size\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n",
    "\n",
    "print(\"To save in GGUF format, uncomment one of the lines above and run the cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b1e37",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We Did:\n",
    "1. ✅ Installed unsloth and dependencies\n",
    "2. ✅ Loaded SmolLM2 135M model with 4-bit quantization\n",
    "3. ✅ Configured for **FULL FINETUNING** (not LoRA)\n",
    "4. ✅ Prepared and formatted Alpaca dataset with chat template\n",
    "5. ✅ Trained ALL 135M parameters\n",
    "6. ✅ Tested the model with inference\n",
    "7. ✅ Saved the model for future use\n",
    "\n",
    "### Full Finetuning vs LoRA:\n",
    "| Aspect | Full Finetuning | LoRA |\n",
    "|--------|----------------|------|\n",
    "| **get_peft_model()** | ❌ Not used | ✅ Used with r=16+ |\n",
    "| **Parameters Updated** | ALL (135M) | Small subset (~1-2M) |\n",
    "| **Memory Usage** | Higher | Lower |\n",
    "| **Training Time** | Longer | Faster |\n",
    "| **Performance** | Better | Good |\n",
    "| **Use Case** | Critical tasks | General tasks |\n",
    "\n",
    "### Dataset Format:\n",
    "```\n",
    "<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_message}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{response}<|im_end|>\n",
    "```\n",
    "\n",
    "### Tips for Better Results:\n",
    "1. **Increase max_steps**: 500-1000 for production\n",
    "2. **More data**: Use more training samples\n",
    "3. **Adjust learning rate**: Experiment with 1e-4 to 5e-4\n",
    "4. **Enable packing**: Set `packing=True` for shorter sequences\n",
    "5. **Monitor loss**: Watch training loss to prevent overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
